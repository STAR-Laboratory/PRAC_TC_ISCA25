{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 14 (Normalized Performance): 5-entry PSQ, 32 NBO, PRAC-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          workload   NBO  QPRAC-NoOp     QPRAC  QPRAC+Proactive\n",
      "0        401.bzip2  32.0    0.902356  0.997772              1.0\n",
      "1          403.gcc  32.0    1.000000  1.000000              1.0\n",
      "2          429.mcf  32.0         NaN       NaN              NaN\n",
      "3         433.milc  32.0    0.948515  0.997549              1.0\n",
      "4       434.zeusmp  32.0    0.736759  0.963384              1.0\n",
      "..             ...   ...         ...       ...              ...\n",
      "59         TPC (4)  32.0    0.889154  0.997550              1.0\n",
      "60      Hadoop (3)  32.0    0.990644  0.999279              1.0\n",
      "61  MediaBench (3)  32.0    0.815713  0.986067              1.0\n",
      "62        YCSB (6)  32.0    0.959188  0.997987              1.0\n",
      "63        All (57)  32.0    0.874504  0.991259              1.0\n",
      "\n",
      "[64 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/QPRAC/4cores/500MInsts/MOP'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"workload\"])\n",
    "mitigation_list = [\"Baseline\", \"QPRAC-NoOp\",\"QPRAC\", \"QPRAC+Proactive\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        NBO = int(result_filename.split(\"_\")[0])\n",
    "        if NBO != 32:\n",
    "            continue\n",
    "        prac_level = int(result_filename.split(\"_\")[1])\n",
    "        if prac_level != 1:\n",
    "            continue\n",
    "        psq_size = int(result_filename.split(\"_\")[2])\n",
    "        if psq_size != 5:\n",
    "            continue\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[4:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        num_abo = 0\n",
    "        num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" controller0_num_refresh_reqs\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        WS = ipc_0 + ipc_1 + ipc_2 + ipc_3\n",
    "        ABO_per_tREFI = float(num_abo)/float(num_tREFI_period/2)\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'workload': [workload],\n",
    "            'mitigation': [mitigation],\n",
    "            'NBO': [NBO],\n",
    "            'WS': [WS],\n",
    "            'ABO_per_tREFI': [ABO_per_tREFI],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_ws = df.pivot(index=['workload', 'NBO'], columns=['mitigation'], values='WS').reset_index()\n",
    "df_abo = df.pivot(index=['workload', 'NBO'], columns=['mitigation'], values='ABO_per_tREFI').reset_index()\n",
    "df_abo.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "    df_ws[mitigation] = df_ws[mitigation] / df_ws['Baseline']\n",
    "df_ws.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "# print(df_ws)\n",
    "\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 23\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for NBO in df['NBO'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['NBO'] == NBO)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'NBO': NBO, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for NBO in df['NBO'].unique():\n",
    "            Channel_interface_df = df[(df['NBO'] == NBO)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'NBO': NBO, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "\n",
    "mitigation_list = [\"QPRAC-NoOp\",\"QPRAC\", \"QPRAC+Proactive\"]\n",
    "new_column_order = ['workload', 'NBO'] + mitigation_list\n",
    "\n",
    "geomean_df = add_geomean_rows(df_ws)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "geomean_df = geomean_df[new_column_order]\n",
    "print(geomean_df)\n",
    "geomean_df.to_csv('../stats/4cores_QPRAC_32NBO_5PSQ_500MInsts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 15 (ABO per tREFI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          workload   NBO  QPRAC-NoOp     QPRAC  QPRAC+Proactive\n",
      "0        401.bzip2  32.0    1.596443  0.056995              0.0\n",
      "1          403.gcc  32.0    0.000000  0.000000              0.0\n",
      "2          429.mcf  32.0         NaN       NaN              NaN\n",
      "3         433.milc  32.0    0.480503  0.023194              0.0\n",
      "4       434.zeusmp  32.0    2.262768  0.316365              0.0\n",
      "..             ...   ...         ...       ...              ...\n",
      "59         TPC (4)  32.0    1.152060  0.036749              0.0\n",
      "60      Hadoop (3)  32.0    0.101241  0.003381              0.0\n",
      "61  MediaBench (3)  32.0    1.549168  0.121235              0.0\n",
      "62        YCSB (6)  32.0    0.487795  0.024992              0.0\n",
      "63        All (57)  32.0    1.155248  0.080120              0.0\n",
      "\n",
      "[64 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate geometric mean\n",
    "def calculate_arithmetic_mean(series):\n",
    "    return series.mean()\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_arithmetic_mean_rows(df):\n",
    "    amean_rows = []  # List to collect new rows\n",
    "\n",
    "    for NBO in df['NBO'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['NBO'] == NBO)]\n",
    "            if not suite_df.empty:\n",
    "                ameans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        ameans[mitigation] = calculate_arithmetic_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                amean_row = {'NBO': NBO, 'workload': suite_name, **ameans}\n",
    "                amean_rows.append(amean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    amean_df = pd.DataFrame(amean_rows)\n",
    "    \n",
    "    return pd.concat([df, amean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_amean_rows(df):\n",
    "    amean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for NBO in df['NBO'].unique():\n",
    "            Channel_interface_df = df[(df['NBO'] == NBO)]\n",
    "            amean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    amean_values[mitigation] = calculate_arithmetic_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            amean_row = {'NBO': NBO, 'workload': 'All (57)', **amean_values}\n",
    "            amean_rows.append(amean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    amean_df = pd.DataFrame(amean_rows)\n",
    "    \n",
    "    return pd.concat([df, amean_df], ignore_index=True)\n",
    "\n",
    "mitigation_list = [\"QPRAC-NoOp\",\"QPRAC\", \"QPRAC+Proactive\"]\n",
    "new_column_order = ['workload', 'NBO'] + mitigation_list\n",
    "\n",
    "amean_df = add_arithmetic_mean_rows(df_abo)\n",
    "amean_df = add_all_workloads_amean_rows(amean_df)\n",
    "amean_df = amean_df[new_column_order]\n",
    "print(amean_df)\n",
    "amean_df.to_csv('../stats/4cores_QPRAC_32NBO_5PSQ_500MInsts_ABO_per_tREFI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 16 (Varying RFMs per ABO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           workload  PRAC_level     QPRAC  QPRAC+Proactive\n",
      "0         401.bzip2         1.0  0.997772              1.0\n",
      "1         401.bzip2         2.0  0.997876              1.0\n",
      "2         401.bzip2         4.0  0.995595              1.0\n",
      "3           403.gcc         1.0  1.000000              1.0\n",
      "4           403.gcc         2.0  1.000000              1.0\n",
      "..              ...         ...       ...              ...\n",
      "184  MediaBench (3)         4.0  0.977043              1.0\n",
      "185        YCSB (6)         4.0  0.995813              1.0\n",
      "186        All (57)         1.0  0.991109              1.0\n",
      "187        All (57)         2.0  0.990693              1.0\n",
      "188        All (57)         4.0  0.988691              1.0\n",
      "\n",
      "[189 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/QPRAC/4cores/500MInsts/MOP'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"workload\"])\n",
    "df_baseline = pd.DataFrame(columns=[\"workload\"])\n",
    "mitigation_list = [\"Baseline\", \"QPRAC\", \"QPRAC+Proactive\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        NBO = int(result_filename.split(\"_\")[0])\n",
    "        if NBO != 32:\n",
    "            continue\n",
    "        prac_level = int(result_filename.split(\"_\")[1])\n",
    "        psq_size = int(result_filename.split(\"_\")[2])\n",
    "        if psq_size != 5:\n",
    "            continue\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[4:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" controller0_num_refresh_reqs\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        WS = ipc_0 + ipc_1 + ipc_2 + ipc_3\n",
    "        ABO_per_tREFI = float(num_abo)/float(num_tREFI_period/2)\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "\n",
    "        if mitigation in ['Baseline']:\n",
    "            baseline_new_row = pd.DataFrame({\n",
    "                'workload': [workload],\n",
    "                'mitigation': [mitigation],\n",
    "                'NBO': [NBO],\n",
    "                'WS': [WS]\n",
    "            })\n",
    "            df_baseline = pd.concat([df_baseline, baseline_new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\n",
    "                'workload': [workload],\n",
    "                'mitigation': [mitigation],\n",
    "                'PRAC_level': [prac_level],\n",
    "                'NBO': [NBO],\n",
    "                'WS': [WS],\n",
    "                'ABO_per_tREFI': [ABO_per_tREFI],\n",
    "            })\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_baseline = df_baseline.pivot(index=['workload'], columns=['mitigation'], values='WS').reset_index()\n",
    "df_ws = df.pivot(index=['workload', 'PRAC_level'], columns=['mitigation'], values='WS').reset_index()\n",
    "\n",
    "df_ws = df_ws.merge(df_baseline, on='workload', how='left')\n",
    "for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "    df_ws[mitigation] = df_ws[mitigation] / df_ws['Baseline']\n",
    "df_ws.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "# print(df_ws)\n",
    "\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 23\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for NBO in df['PRAC_level'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['PRAC_level'] == NBO)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'PRAC_level': NBO, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for NBO in df['PRAC_level'].unique():\n",
    "            Channel_interface_df = df[(df['PRAC_level'] == NBO)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'PRAC_level': NBO, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "\n",
    "mitigation_list = [\"QPRAC\", \"QPRAC+Proactive\"]\n",
    "new_column_order = ['workload', 'PRAC_level'] + mitigation_list\n",
    "\n",
    "geomean_df = add_geomean_rows(df_ws)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "geomean_df = geomean_df[new_column_order]\n",
    "print(geomean_df)\n",
    "geomean_df.to_csv('../stats/4cores_QPRAC_32NBO_5PSQ_500MInsts_varying_RFMs_per_ABO_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 17 (Varying PSQ sizes with varying Targeted Refresh ratio)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      workload  PSQ_size     QPRAC  QPRAC+1Proactive_per_4tREFI  \\\n",
      "0    401.bzip2       1.0  0.969013                     1.000000   \n",
      "1    401.bzip2       2.0  0.996357                     1.000000   \n",
      "2    401.bzip2       3.0  0.991076                     1.000000   \n",
      "3    401.bzip2       4.0  0.998469                     1.000000   \n",
      "4    401.bzip2       5.0  0.997772                     1.000000   \n",
      "..         ...       ...       ...                          ...   \n",
      "310   All (57)       1.0  0.937153                     0.972831   \n",
      "311   All (57)       2.0  0.986540                     0.994966   \n",
      "312   All (57)       3.0  0.989529                     0.996934   \n",
      "313   All (57)       4.0  0.990682                     0.997509   \n",
      "314   All (57)       5.0  0.991109                     0.998440   \n",
      "\n",
      "     QPRAC+1Proactive_per_2tREFI  QPRAC+1Proactive_per_1tREFI  \n",
      "0                       1.000000                          1.0  \n",
      "1                       1.000000                          1.0  \n",
      "2                       1.000000                          1.0  \n",
      "3                       1.000000                          1.0  \n",
      "4                       1.000000                          1.0  \n",
      "..                           ...                          ...  \n",
      "310                     0.998136                          1.0  \n",
      "311                     0.999992                          1.0  \n",
      "312                     0.999832                          1.0  \n",
      "313                     0.999998                          1.0  \n",
      "314                     0.999968                          1.0  \n",
      "\n",
      "[315 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/QPRAC/4cores/500MInsts/MOP'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"workload\"])\n",
    "df_baseline = pd.DataFrame(columns=[\"workload\"])\n",
    "mitigation_list = [\"Baseline\", \"QPRAC\", \"QPRAC+1Proactive_per_4tREFI\", \"QPRAC+1Proactive_per_2tREFI\", \"QPRAC+1Proactive_per_1tREFI\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        NBO = int(result_filename.split(\"_\")[0])\n",
    "        if NBO != 32:\n",
    "            continue\n",
    "        prac_level = int(result_filename.split(\"_\")[1])\n",
    "        if prac_level != 1:\n",
    "            continue\n",
    "        psq_size = int(result_filename.split(\"_\")[2])\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[4:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" controller0_num_refresh_reqs\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        WS = ipc_0 + ipc_1 + ipc_2 + ipc_3\n",
    "        ABO_per_tREFI = float(num_abo)/float(num_tREFI_period/2)\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "\n",
    "        if mitigation in ['Baseline']:\n",
    "            baseline_new_row = pd.DataFrame({\n",
    "                'workload': [workload],\n",
    "                'mitigation': [mitigation],\n",
    "                'NBO': [NBO],\n",
    "                'WS': [WS]\n",
    "            })\n",
    "            df_baseline = pd.concat([df_baseline, baseline_new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\n",
    "                'workload': [workload],\n",
    "                'mitigation': [mitigation],\n",
    "                'PSQ_size': [psq_size],\n",
    "                'NBO': [NBO],\n",
    "                'WS': [WS],\n",
    "                'ABO_per_tREFI': [ABO_per_tREFI],\n",
    "            })\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_baseline = df_baseline.pivot(index=['workload'], columns=['mitigation'], values='WS').reset_index()\n",
    "df_ws = df.pivot(index=['workload', 'PSQ_size'], columns=['mitigation'], values='WS').reset_index()\n",
    "\n",
    "df_ws = df_ws.merge(df_baseline, on='workload', how='left')\n",
    "for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "    df_ws[mitigation] = df_ws[mitigation] / df_ws['Baseline']\n",
    "df_ws.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "# print(df_ws)\n",
    "\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 23\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for NBO in df['PSQ_size'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['PSQ_size'] == NBO)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'PSQ_size': NBO, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for NBO in df['PSQ_size'].unique():\n",
    "            Channel_interface_df = df[(df['PSQ_size'] == NBO)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'PSQ_size': NBO, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "\n",
    "mitigation_list = [\"QPRAC\", \"QPRAC+1Proactive_per_4tREFI\", \"QPRAC+1Proactive_per_2tREFI\", \"QPRAC+1Proactive_per_1tREFI\"]\n",
    "new_column_order = ['workload', 'PSQ_size'] + mitigation_list\n",
    "\n",
    "geomean_df = add_geomean_rows(df_ws)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "geomean_df = geomean_df[new_column_order]\n",
    "print(geomean_df)\n",
    "geomean_df.to_csv('../stats/4cores_QPRAC_32NBO_500MInsts_varying_PSQ_sizes_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**352 ROB OPT MC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           workload  Channel  interface  Baseline  Baseline-ClosedCap4  \\\n",
      "0         401.bzip2      1.0     3200.0  3.291958             3.294998   \n",
      "1         401.bzip2      1.0     6400.0  3.381844             3.382914   \n",
      "2         401.bzip2      1.0     8800.0  3.401019             3.403233   \n",
      "3         401.bzip2      1.0    12800.0  3.411890             3.412616   \n",
      "4         401.bzip2      1.0    17600.0  3.426491             3.427592   \n",
      "..              ...      ...        ...       ...                  ...   \n",
      "562   SPEC2K17 (18)     16.0     8800.0       NaN                  NaN   \n",
      "563         TPC (4)     16.0     8800.0       NaN                  NaN   \n",
      "564      Hadoop (3)     16.0     8800.0       NaN                  NaN   \n",
      "565  MediaBench (3)     16.0     8800.0       NaN                  NaN   \n",
      "566        YCSB (6)     16.0     8800.0       NaN                  NaN   \n",
      "\n",
      "     PRAC_WO_Mitigation  PRAC_WO_Mitigation-ClosedCap4  \\\n",
      "0              3.218930                       3.221951   \n",
      "1              3.288965                       3.294473   \n",
      "2              3.313409                       3.317480   \n",
      "3              3.322674                       3.327497   \n",
      "4              3.330714                       3.336275   \n",
      "..                  ...                            ...   \n",
      "562                 NaN                            NaN   \n",
      "563                 NaN                            NaN   \n",
      "564                 NaN                            NaN   \n",
      "565                 NaN                            NaN   \n",
      "566                 NaN                            NaN   \n",
      "\n",
      "     Normalized_PRAC_WO_Mitigation  Normalized_PRAC_WO_Mitigation-ClosedCap4  \n",
      "0                         0.977816                                  0.977831  \n",
      "1                         0.972536                                  0.973857  \n",
      "2                         0.974240                                  0.974803  \n",
      "3                         0.973851                                  0.975057  \n",
      "4                         0.972048                                  0.973358  \n",
      "..                             ...                                       ...  \n",
      "562                       0.978021                                  1.000000  \n",
      "563                       0.980569                                  1.000000  \n",
      "564                       0.984604                                  1.000000  \n",
      "565                       0.995630                                  1.000000  \n",
      "566                       0.989198                                  1.000000  \n",
      "\n",
      "[567 rows x 9 columns]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 172\u001b[0m\n\u001b[1;32m    170\u001b[0m geomean_df \u001b[38;5;241m=\u001b[39m add_geomean_rows(df_baseline)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(geomean_df)\n\u001b[0;32m--> 172\u001b[0m geomean_df \u001b[38;5;241m=\u001b[39m \u001b[43madd_all_workloads_geomean_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeomean_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m geomean_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized_PRAC_WO_Mitigation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized_PRAC_WO_Mitigation-ClosedCap4\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../stats/SC_500M_OPTMC_result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 159\u001b[0m, in \u001b[0;36madd_all_workloads_geomean_rows\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mitigation \u001b[38;5;129;01min\u001b[39;00m mitigation_list:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mitigation \u001b[38;5;129;01min\u001b[39;00m Channel_interface_df\u001b[38;5;241m.\u001b[39mcolumns:  \u001b[38;5;66;03m# Ensure the column exists\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m         geomean_values[mitigation] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_geometric_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChannel_interface_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmitigation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Create a new row for the combined results\u001b[39;00m\n\u001b[1;32m    162\u001b[0m geomean_row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel\u001b[39m\u001b[38;5;124m'\u001b[39m: Channel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterface\u001b[39m\u001b[38;5;124m'\u001b[39m: interface, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll (57)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeomean_values}\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mcalculate_geometric_mean\u001b[0;34m(series)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_geometric_mean\u001b[39m(series):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprod(series) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "baseline_4cores_out_path = '../../prac_study/TimingOverhead/SC/OPTMC/352rob'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\", \"WS\"])\n",
    "mitigation_list = [\"Baseline\", \"PRAC_WO_Mitigation\", \n",
    "                   \"Baseline-ClosedCap4\", \"PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = baseline_4cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        ipc_0 = 0\n",
    "        cycle_0 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_abo=0\n",
    "        num_tREFI_period=0\n",
    "        num_tREFW_period=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_command_0\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_window_0\" in line):\n",
    "                num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "\n",
    "        if (cycle_0 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 ):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'WS': [ipc_0],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df[['workload', 'Channel', 'interface', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./../stats/SC_500M_OPTMC_workload_characteristics.csv', index=False)\n",
    "df_baseline = df.pivot(index=['workload', 'Channel', 'interface'], columns=['mitigation'], values='WS').reset_index()\n",
    "df_baseline[['workload', 'Channel','interface', 'Baseline','Baseline-ClosedCap4']].to_csv('./../stats/SC_500M_OPTMC_Baseline.csv', index=False)\n",
    "\n",
    "# List of PRAC_WO_Mitigation columns and their corresponding Baseline columns\n",
    "prac_columns = [\n",
    "    'PRAC_WO_Mitigation',\n",
    "    # 'PRAC_WO_Mitigation-ClosedCap1',\n",
    "    'PRAC_WO_Mitigation-ClosedCap4'\n",
    "]\n",
    "\n",
    "baseline_columns = [\n",
    "    'Baseline',\n",
    "    # 'Baseline-ClosedCap1',\n",
    "    'Baseline-ClosedCap4'\n",
    "]\n",
    "\n",
    "# Loop through each pair of PRAC and Baseline columns to create normalized columns\n",
    "for prac_col, baseline_col in zip(prac_columns, baseline_columns):\n",
    "    normalized_col_name = f'Normalized_{prac_col}'  # Name of the new normalized column\n",
    "    df_baseline[normalized_col_name] = df_baseline[prac_col] / df_baseline[baseline_col]  # Calculate normalized values\n",
    "\n",
    "# print(df_baseline)\n",
    "\n",
    "# workload_interests = ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '482.sphinx3', '483.xalancbmk', \n",
    "#                       '500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz',\n",
    "#                       'tpcc64', 'tpch17', 'tpch2', 'tpch6',\n",
    "#                       'grep_map0', 'wc_8443', 'wc_map0',\n",
    "#                       'h264_encode', 'jp2_decode', 'jp2_encode',\n",
    "#                       'ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver',\n",
    "#                       'random_10.trace', 'stream_10.trace'\n",
    "#                       ]\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 23\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "#\n",
    "mitigation_list = [\"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap1\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            for suite_name, workloads in benchmark_suites.items():\n",
    "                suite_df = df[(df['workload'].isin(workloads)) & (df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "                if not suite_df.empty:\n",
    "                    geomeans = {}\n",
    "                    \n",
    "                    # Dynamically calculate geometric means for each mitigation\n",
    "                    for mitigation in mitigation_list:\n",
    "                        if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                            geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                    \n",
    "                    # Create a new row\n",
    "                    geomean_row = {'Channel': Channel, 'interface': interface, 'workload': suite_name, **geomeans}\n",
    "                    geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            Channel_interface_df = df[(df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'Channel': Channel, 'interface': interface, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "# Call function to calculate and merge geometric means\n",
    "geomean_df = add_geomean_rows(df_baseline)\n",
    "print(geomean_df)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'interface', \"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]].to_csv('./../stats/SC_500M_OPTMC_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Core, MOP, 500M, Open Page, Varying Channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     workload  Channel  PRAC_WO_Mitigation  Mix_index Benchmark_Types\n",
      "0        MIX0      1.0            0.857117        0.0       HHHH (15)\n",
      "1        MIX0      2.0            0.842962        0.0       HHHH (15)\n",
      "2        MIX0      4.0            0.846322        0.0       HHHH (15)\n",
      "3        MIX0      8.0            0.868094        0.0       HHHH (15)\n",
      "4        MIX0     16.0            0.875079        0.0       HHHH (15)\n",
      "..        ...      ...                 ...        ...             ...\n",
      "479  All (90)      1.0            0.901812        NaN             NaN\n",
      "480  All (90)      2.0            0.905765        NaN             NaN\n",
      "481  All (90)      4.0            0.919181        NaN             NaN\n",
      "482  All (90)      8.0            0.931679        NaN             NaN\n",
      "483  All (90)     16.0            0.941217        NaN             NaN\n",
      "\n",
      "[484 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/TimingOverhead/4cores_results/OPTMC'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"Baseline\",\"PRAC_WO_Mitigation\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "        if interface != 8800:\n",
    "            continue\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'wl0': [w0],\n",
    "            'wl1': [w1],\n",
    "            'wl2': [w2],\n",
    "            'wl3': [w3],\n",
    "            'ipc0': [ipc_0],\n",
    "            'ipc1': [ipc_1],\n",
    "            'ipc2': [ipc_2],\n",
    "            'ipc3': [ipc_3],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df)\n",
    "df[['workload', 'interface', 'Channel', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./../stats/4core_OPTMC_heterogeneous_workload_characteristics.csv', index=False)\n",
    "\n",
    "df_sc_ipc = pd.read_csv('../stats/SC_500M_OPTMC_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['interface'] == 8800)]\n",
    "# df_sc_ipc = df_sc_ipc.drop(columns=['interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline']], left_on=['Channel', 'wl0'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline']], left_on=['Channel', 'wl1'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline']], left_on=['Channel', 'wl2'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline']], left_on=['Channel', 'wl3'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_closed_cap1_ws = df[['mitigation', 'workload', 'Channel', 'WS']]\n",
    "\n",
    "df_closed_cap1_ws_pivot = df_closed_cap1_ws.pivot(index=['workload', 'Channel'], columns=['mitigation'], values='WS').reset_index()\n",
    "for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "     df_closed_cap1_ws_pivot[mitigation] = df_closed_cap1_ws_pivot[mitigation] / df_closed_cap1_ws_pivot['Baseline']\n",
    "df_closed_cap1_ws_pivot.drop(columns=['Baseline'], inplace=True)\n",
    "##### Calculate the Geomean for each workload type\n",
    "# Define benchmark suites and their corresponding workloads ranges\n",
    "benchmark_types = {\n",
    "    'HHHH (15)': list(range(0, 15)),   # From Mix0-Mix14\n",
    "    'MMMM (15)': list(range(15, 30)),  # From Mix15-Mix29\n",
    "    'LLLL (15)': list(range(30, 45)),  # From Mix30-Mix44\n",
    "    'HHMM (15)': list(range(45, 60)),  # From Mix45-Mix59\n",
    "    'HHLL (15)': list(range(60, 75)),  # From Mix60-Mix74\n",
    "    'MMLL (15)': list(range(75, 90)),  # From Mix75-Mix89\n",
    "}\n",
    "\n",
    "# DataFrame Example (you already have your df)\n",
    "# Assuming your column of interest is 'PRAC_WO_Mitigation-ClosedCap1'\n",
    "\n",
    "# Add a new column to assign each Mix to the appropriate suite\n",
    "def assign_benchmark_types(mix_index):\n",
    "    for suite, mix_range in benchmark_types.items():\n",
    "        if mix_index in mix_range:\n",
    "            return suite\n",
    "    return None\n",
    "\n",
    "# Assuming 'workload' has values like 'MIX0', 'MIX1', etc., you can extract the index\n",
    "df_closed_cap1_ws_pivot['Mix_index'] = df_closed_cap1_ws_pivot['workload'].str.extract(r'(\\d+)').astype(int)  # Extract Mix number\n",
    "df_closed_cap1_ws_pivot['Benchmark_Types'] = df_closed_cap1_ws_pivot['Mix_index'].apply(assign_benchmark_types)  # Assign benchmark suite\n",
    "\n",
    "# Function to calculate the geometric mean\n",
    "def geom_mean(series):\n",
    "    return np.exp(np.log(series).mean())\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for suite_name, mix_indices in benchmark_types.items():\n",
    "            # Create a list of corresponding workload names (e.g., MIX0, MIX1)\n",
    "            workloads = [f'MIX{i}' for i in mix_indices]\n",
    "            suite_df = df[df['workload'].isin(workloads) & (df['Channel'] == Channel)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'Channel': Channel, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        Channel_df = df[df['Channel'] == Channel]\n",
    "        geomean_values = {}\n",
    "        \n",
    "        # Calculate geometric means for each mitigation in the list\n",
    "        for mitigation in mitigation_list:\n",
    "            if mitigation in Channel_df.columns:  # Ensure the column exists\n",
    "                geomean_values[mitigation] = calculate_geometric_mean(Channel_df[mitigation])\n",
    "\n",
    "        # Create a new row for the combined results\n",
    "        geomean_row = {'Channel': Channel, 'workload': 'All (90)', **geomean_values}\n",
    "        geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "mitigation_list = [\"PRAC_WO_Mitigation\"]\n",
    "\n",
    "geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'PRAC_WO_Mitigation', \"Benchmark_Types\"]].to_csv('../stats/4cores_OPTMC_DDR58800_500M_Heterogeneous_OpenPage_varying_channels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Core, MOP, 500M, Open Page, Varying Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     workload  Channel  interface  PRAC_WO_Mitigation  Mix_index  \\\n",
      "0        MIX0      1.0     3200.0            0.973599        0.0   \n",
      "1        MIX0      1.0     6400.0            0.886445        0.0   \n",
      "2        MIX0      1.0     8800.0            0.858497        0.0   \n",
      "3        MIX0      1.0    12800.0            0.848931        0.0   \n",
      "4        MIX0      1.0    17600.0            0.844092        0.0   \n",
      "..        ...      ...        ...                 ...        ...   \n",
      "475  All (57)      1.0     3200.0            0.964069        NaN   \n",
      "476  All (57)      1.0     6400.0            0.922787        NaN   \n",
      "477  All (57)      1.0     8800.0            0.908726        NaN   \n",
      "478  All (57)      1.0    12800.0            0.902723        NaN   \n",
      "479  All (57)      1.0    17600.0            0.901645        NaN   \n",
      "\n",
      "    Benchmark_Types  \n",
      "0         HHHH (15)  \n",
      "1         HHHH (15)  \n",
      "2         HHHH (15)  \n",
      "3         HHHH (15)  \n",
      "4         HHHH (15)  \n",
      "..              ...  \n",
      "475             NaN  \n",
      "476             NaN  \n",
      "477             NaN  \n",
      "478             NaN  \n",
      "479             NaN  \n",
      "\n",
      "[480 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/TimingOverhead/4cores_results/OPTMC'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"Baseline\", \"PRAC_WO_Mitigation\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "        if Channel != 1:\n",
    "            continue\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'wl0': [w0],\n",
    "            'wl1': [w1],\n",
    "            'wl2': [w2],\n",
    "            'wl3': [w3],\n",
    "            'ipc0': [ipc_0],\n",
    "            'ipc1': [ipc_1],\n",
    "            'ipc2': [ipc_2],\n",
    "            'ipc3': [ipc_3],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df)\n",
    "df[['workload', 'interface', 'Channel', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./../stats/4core_OPTMC_heterogeneous_workload_characteristics.csv', index=False)\n",
    "\n",
    "df_sc_ipc = pd.read_csv('../stats/SC_500M_OPTMC_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel'] == 1)]\n",
    "# df_sc_ipc = df_sc_ipc.drop(columns=['interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "# Calculate normalized IPC for 'Baseline' and 'PRAC_WO_Mitigation' using ipc_wl\n",
    "df['normalized_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalized_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalized_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalized_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "# Calculate WS for each set\n",
    "df['WS'] = df[['normalized_ipc0', 'normalized_ipc1', 'normalized_ipc2', 'normalized_ipc3']].sum(axis=1)\n",
    "\n",
    "\n",
    "df_open_varying_interface_ws = df[['mitigation', 'workload', 'Channel', 'interface', 'WS']]\n",
    "\n",
    "# Pivot WS columns\n",
    "df_open_varying_interface_ws_pivot = df_open_varying_interface_ws.pivot(index=['workload', 'Channel', 'interface'], columns='mitigation', values='WS').reset_index()\n",
    "\n",
    "# Normalize WS by 'Baseline'\n",
    "for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "    df_open_varying_interface_ws_pivot[mitigation] = df_open_varying_interface_ws_pivot[mitigation] / df_open_varying_interface_ws_pivot['Baseline']\n",
    "\n",
    "# Drop Baseline columns\n",
    "df_open_varying_interface_ws_pivot.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "# Merge results back if needed\n",
    "##### Calculate the Geomean for each workload type\n",
    "# Define benchmark suites and their corresponding workloads ranges\n",
    "benchmark_types = {\n",
    "    'HHHH (15)': list(range(0, 15)),   # From Mix0-Mix14\n",
    "    'MMMM (15)': list(range(15, 30)),  # From Mix15-Mix29\n",
    "    'LLLL (15)': list(range(30, 45)),  # From Mix30-Mix44\n",
    "    'HHMM (15)': list(range(45, 60)),  # From Mix45-Mix59\n",
    "    'HHLL (15)': list(range(60, 75)),  # From Mix60-Mix74\n",
    "    'MMLL (15)': list(range(75, 90)),  # From Mix75-Mix89\n",
    "}\n",
    "\n",
    "# DataFrame Example (you already have your df)\n",
    "# Assuming your column of interest is 'PRAC_WO_Mitigation-ClosedCap1'\n",
    "\n",
    "# Add a new column to assign each Mix to the appropriate suite\n",
    "def assign_benchmark_types(mix_index):\n",
    "    for suite, mix_range in benchmark_types.items():\n",
    "        if mix_index in mix_range:\n",
    "            return suite\n",
    "    return None\n",
    "\n",
    "# Assuming 'workload' has values like 'MIX0', 'MIX1', etc., you can extract the index\n",
    "df_open_varying_interface_ws_pivot['Mix_index'] = df_open_varying_interface_ws_pivot['workload'].str.extract(r'(\\d+)').astype(int)  # Extract Mix number\n",
    "df_open_varying_interface_ws_pivot['Benchmark_Types'] = df_open_varying_interface_ws_pivot['Mix_index'].apply(assign_benchmark_types)  # Assign benchmark suite\n",
    "\n",
    "# Function to calculate the geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.exp(np.log(series).mean())\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            for suite_name, mix_indices in benchmark_types.items():\n",
    "                # Create a list of corresponding workload names (e.g., MIX0, MIX1)\n",
    "                workloads = [f'MIX{i}' for i in mix_indices]\n",
    "                suite_df = df[df['workload'].isin(workloads) & (df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "                if not suite_df.empty:\n",
    "                    geomeans = {}\n",
    "                    \n",
    "                    # Dynamically calculate geometric means for each mitigation\n",
    "                    for mitigation in mitigation_list:\n",
    "                        if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                            geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                    \n",
    "                    # Create a new row\n",
    "                    geomean_row = {'Channel': Channel, 'interface': interface, 'workload': suite_name, **geomeans}\n",
    "                    geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            Channel_interface_df = df[(df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "            geomean_values = {}\n",
    "            \n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'Channel': Channel, 'interface': interface, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "mitigation_list = [\"PRAC_WO_Mitigation\"]\n",
    "\n",
    "geomean_df = add_geomean_rows(df_open_varying_interface_ws_pivot)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'interface', 'PRAC_WO_Mitigation', \"Benchmark_Types\"]].to_csv('../stats/4cores_OPTMC_OpenPage_500M_Heterogeneous_varying_interface.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/TimingOverhead/4cores_results/OPTMC'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"Baseline-ClosedCap4\", \"PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "        if Channel != 1:\n",
    "            continue\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'wl0': [w0],\n",
    "            'wl1': [w1],\n",
    "            'wl2': [w2],\n",
    "            'wl3': [w3],\n",
    "            'ipc0': [ipc_0],\n",
    "            'ipc1': [ipc_1],\n",
    "            'ipc2': [ipc_2],\n",
    "            'ipc3': [ipc_3],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df)\n",
    "df[['workload', 'interface', 'Channel', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./../stats/4core_OPTMC_heterogeneous_workload_characteristics.csv', index=False)\n",
    "\n",
    "df_sc_ipc = pd.read_csv('../stats/SC_500M_OPTMC_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline-ClosedCap4']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel'] == 1)]\n",
    "# df_sc_ipc = df_sc_ipc.drop(columns=['interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline-ClosedCap4']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap4': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline-ClosedCap4']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap4': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline-ClosedCap4']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap4': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'interface', 'workload_sc', 'Baseline-ClosedCap4']], left_on=['Channel', 'interface', 'wl0'], right_on=['Channel', 'interface', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap4': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "# Calculate normalized IPC for 'Baseline' and 'PRAC_WO_Mitigation' using ipc_wl\n",
    "df['normalized_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalized_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalized_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalized_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "# Calculate WS for each set\n",
    "df['WS'] = df[['normalized_ipc0', 'normalized_ipc1', 'normalized_ipc2', 'normalized_ipc3']].sum(axis=1)\n",
    "\n",
    "\n",
    "df_open_varying_interface_ws = df[['mitigation', 'workload', 'Channel', 'interface', 'WS']]\n",
    "\n",
    "# Pivot WS columns\n",
    "df_open_varying_interface_ws_pivot = df_open_varying_interface_ws.pivot(index=['workload', 'Channel', 'interface'], columns='mitigation', values='WS').reset_index()\n",
    "\n",
    "# Normalize WS by 'Baseline'\n",
    "for mitigation in set(mitigation_list) - set(['Baseline-ClosedCap4']):\n",
    "    df_open_varying_interface_ws_pivot[mitigation] = df_open_varying_interface_ws_pivot[mitigation] / df_open_varying_interface_ws_pivot['Baseline-ClosedCap4']\n",
    "\n",
    "# Drop Baseline columns\n",
    "df_open_varying_interface_ws_pivot.drop(columns=['Baseline-ClosedCap4'], inplace=True)\n",
    "\n",
    "# Merge results back if needed\n",
    "##### Calculate the Geomean for each workload type\n",
    "# Define benchmark suites and their corresponding workloads ranges\n",
    "benchmark_types = {\n",
    "    'HHHH (15)': list(range(0, 15)),   # From Mix0-Mix14\n",
    "    'MMMM (15)': list(range(15, 30)),  # From Mix15-Mix29\n",
    "    'LLLL (15)': list(range(30, 45)),  # From Mix30-Mix44\n",
    "    'HHMM (15)': list(range(45, 60)),  # From Mix45-Mix59\n",
    "    'HHLL (15)': list(range(60, 75)),  # From Mix60-Mix74\n",
    "    'MMLL (15)': list(range(75, 90)),  # From Mix75-Mix89\n",
    "}\n",
    "\n",
    "# DataFrame Example (you already have your df)\n",
    "# Assuming your column of interest is 'PRAC_WO_Mitigation-ClosedCap1'\n",
    "\n",
    "# Add a new column to assign each Mix to the appropriate suite\n",
    "def assign_benchmark_types(mix_index):\n",
    "    for suite, mix_range in benchmark_types.items():\n",
    "        if mix_index in mix_range:\n",
    "            return suite\n",
    "    return None\n",
    "\n",
    "# Assuming 'workload' has values like 'MIX0', 'MIX1', etc., you can extract the index\n",
    "df_open_varying_interface_ws_pivot['Mix_index'] = df_open_varying_interface_ws_pivot['workload'].str.extract(r'(\\d+)').astype(int)  # Extract Mix number\n",
    "df_open_varying_interface_ws_pivot['Benchmark_Types'] = df_open_varying_interface_ws_pivot['Mix_index'].apply(assign_benchmark_types)  # Assign benchmark suite\n",
    "\n",
    "# Function to calculate the geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.exp(np.log(series).mean())\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            for suite_name, mix_indices in benchmark_types.items():\n",
    "                # Create a list of corresponding workload names (e.g., MIX0, MIX1)\n",
    "                workloads = [f'MIX{i}' for i in mix_indices]\n",
    "                suite_df = df[df['workload'].isin(workloads) & (df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "                if not suite_df.empty:\n",
    "                    geomeans = {}\n",
    "                    \n",
    "                    # Dynamically calculate geometric means for each mitigation\n",
    "                    for mitigation in mitigation_list:\n",
    "                        if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                            geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                    \n",
    "                    # Create a new row\n",
    "                    geomean_row = {'Channel': Channel, 'interface': interface, 'workload': suite_name, **geomeans}\n",
    "                    geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            Channel_interface_df = df[(df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "            geomean_values = {}\n",
    "            \n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'Channel': Channel, 'interface': interface, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "mitigation_list = [\"PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "\n",
    "geomean_df = add_geomean_rows(df_open_varying_interface_ws_pivot)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'interface', 'PRAC_WO_Mitigation-ClosedCap4', \"Benchmark_Types\"]].to_csv('../stats/4cores_OPTMC_ClosedCap4_500M_Heterogeneous_varying_interface.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**128 ROB NonOPT MC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           workload  Channel  interface  Baseline  Baseline-ClosedCap1  \\\n",
      "0         401.bzip2      1.0     3200.0  2.815950             2.618095   \n",
      "1         401.bzip2      1.0     6400.0  2.880786             2.717947   \n",
      "2           403.gcc      1.0     3200.0  3.680948             3.688203   \n",
      "3           403.gcc      1.0     6400.0  3.705629             3.714663   \n",
      "4           429.mcf      1.0     3200.0  0.385007             0.312998   \n",
      "..              ...      ...        ...       ...                  ...   \n",
      "123      Hadoop (3)      1.0     6400.0       NaN                  NaN   \n",
      "124  MediaBench (3)      1.0     6400.0       NaN                  NaN   \n",
      "125        YCSB (6)      1.0     6400.0       NaN                  NaN   \n",
      "126        All (56)      1.0     3200.0       NaN                  NaN   \n",
      "127        All (56)      1.0     6400.0       NaN                  NaN   \n",
      "\n",
      "     Baseline-ClosedCap4  PRAC_WO_Mitigation  PRAC_WO_Mitigation-ClosedCap1  \\\n",
      "0               2.812995            2.714538                       2.575474   \n",
      "1               2.878718            2.772007                       2.662863   \n",
      "2               3.680948            3.647554                       3.682630   \n",
      "3               3.705629            3.667193                       3.705628   \n",
      "4               0.381249            0.319341                       0.290062   \n",
      "..                   ...                 ...                            ...   \n",
      "123                  NaN                 NaN                            NaN   \n",
      "124                  NaN                 NaN                            NaN   \n",
      "125                  NaN                 NaN                            NaN   \n",
      "126                  NaN                 NaN                            NaN   \n",
      "127                  NaN                 NaN                            NaN   \n",
      "\n",
      "     PRAC_WO_Mitigation-ClosedCap4  Normalized_PRAC_WO_Mitigation  \\\n",
      "0                         2.713392                       0.963987   \n",
      "1                         2.770572                       0.962240   \n",
      "2                         3.647554                       0.990928   \n",
      "3                         3.667193                       0.989628   \n",
      "4                         0.317042                       0.829443   \n",
      "..                             ...                            ...   \n",
      "123                            NaN                       0.924047   \n",
      "124                            NaN                       0.946714   \n",
      "125                            NaN                       0.946059   \n",
      "126                            NaN                       0.936132   \n",
      "127                            NaN                       0.926637   \n",
      "\n",
      "     Normalized_PRAC_WO_Mitigation-ClosedCap1  \\\n",
      "0                                    0.983720   \n",
      "1                                    0.979733   \n",
      "2                                    0.998489   \n",
      "3                                    0.997568   \n",
      "4                                    0.926722   \n",
      "..                                        ...   \n",
      "123                                  0.838209   \n",
      "124                                  0.950671   \n",
      "125                                  0.987044   \n",
      "126                                  0.960119   \n",
      "127                                  0.944175   \n",
      "\n",
      "     Normalized_PRAC_WO_Mitigation-ClosedCap4  \n",
      "0                                    0.964592  \n",
      "1                                    0.962432  \n",
      "2                                    0.990928  \n",
      "3                                    0.989628  \n",
      "4                                    0.831588  \n",
      "..                                        ...  \n",
      "123                                  0.952277  \n",
      "124                                  0.948430  \n",
      "125                                  0.946675  \n",
      "126                                  0.939498  \n",
      "127                                  0.931465  \n",
      "\n",
      "[128 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "baseline_4cores_out_path = '../TimingOverhead/SC'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\", \"WS\"])\n",
    "mitigation_list = [\"Baseline\", \"PRAC_WO_Mitigation\", \n",
    "                   \"Baseline-ClosedCap4\", \"PRAC_WO_Mitigation-ClosedCap4\",\n",
    "                   \"Baseline-ClosedCap1\", \"PRAC_WO_Mitigation-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = baseline_4cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        ipc_0 = 0\n",
    "        cycle_0 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_abo=0\n",
    "        num_tREFI_period=0\n",
    "        num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_command_0\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_window_0\" in line):\n",
    "                num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 ):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'WS': [ipc_0]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_baseline = df.pivot(index=['workload', 'Channel', 'interface'], columns=['mitigation'], values='WS').reset_index()\n",
    "\n",
    "# List of PRAC_WO_Mitigation columns and their corresponding Baseline columns\n",
    "prac_columns = [\n",
    "    'PRAC_WO_Mitigation',\n",
    "    'PRAC_WO_Mitigation-ClosedCap1',\n",
    "    'PRAC_WO_Mitigation-ClosedCap4'\n",
    "]\n",
    "\n",
    "baseline_columns = [\n",
    "    'Baseline',\n",
    "    'Baseline-ClosedCap1',\n",
    "    'Baseline-ClosedCap4'\n",
    "]\n",
    "\n",
    "# Loop through each pair of PRAC and Baseline columns to create normalized columns\n",
    "for prac_col, baseline_col in zip(prac_columns, baseline_columns):\n",
    "    normalized_col_name = f'Normalized_{prac_col}'  # Name of the new normalized column\n",
    "    df_baseline[normalized_col_name] = df_baseline[prac_col] / df_baseline[baseline_col]  # Calculate normalized values\n",
    "\n",
    "# print(df_baseline)\n",
    "\n",
    "# workload_interests = ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '482.sphinx3', '483.xalancbmk', \n",
    "#                       '500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz',\n",
    "#                       'tpcc64', 'tpch17', 'tpch2', 'tpch6',\n",
    "#                       'grep_map0', 'wc_8443', 'wc_map0',\n",
    "#                       'h264_encode', 'jp2_decode', 'jp2_encode',\n",
    "#                       'ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver',\n",
    "#                       'random_10.trace', 'stream_10.trace'\n",
    "#                       ]\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (22)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 22\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "#\n",
    "mitigation_list = [\"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap1\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            for suite_name, workloads in benchmark_suites.items():\n",
    "                suite_df = df[(df['workload'].isin(workloads)) & (df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "                if not suite_df.empty:\n",
    "                    geomeans = {}\n",
    "                    \n",
    "                    # Dynamically calculate geometric means for each mitigation\n",
    "                    for mitigation in mitigation_list:\n",
    "                        if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                            geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                    \n",
    "                    # Create a new row\n",
    "                    geomean_row = {'Channel': Channel, 'interface': interface, 'workload': suite_name, **geomeans}\n",
    "                    geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            Channel_interface_df = df[(df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'Channel': Channel, 'interface': interface, 'workload': 'All (56)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "# Call function to calculate and merge geometric means\n",
    "geomean_df = add_geomean_rows(df_baseline)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'interface', \"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap1\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]].to_csv('./SC_500M_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**352 ROB NonOPT MC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           workload  Channel  interface  Baseline  Baseline-ClosedCap4  \\\n",
      "0         401.bzip2      1.0     3200.0  3.283145             3.278445   \n",
      "1         401.bzip2      1.0     6400.0  3.369152             3.364250   \n",
      "2         401.bzip2      1.0     8800.0  3.392935             3.387190   \n",
      "3         401.bzip2      1.0    12800.0  3.403206             3.397320   \n",
      "4         401.bzip2      1.0    17600.0  3.416012             3.411828   \n",
      "..              ...      ...        ...       ...                  ...   \n",
      "562   SPEC2K17 (18)     16.0     8800.0       NaN                  NaN   \n",
      "563         TPC (4)     16.0     8800.0       NaN                  NaN   \n",
      "564      Hadoop (3)     16.0     8800.0       NaN                  NaN   \n",
      "565  MediaBench (3)     16.0     8800.0       NaN                  NaN   \n",
      "566        YCSB (6)     16.0     8800.0       NaN                  NaN   \n",
      "\n",
      "     PRAC_WO_Mitigation  PRAC_WO_Mitigation-ClosedCap4  \\\n",
      "0              3.197581                       3.197263   \n",
      "1              3.266389                       3.268050   \n",
      "2              3.290330                       3.293563   \n",
      "3              3.298420                       3.299776   \n",
      "4              3.308648                       3.310361   \n",
      "..                  ...                            ...   \n",
      "562                 NaN                            NaN   \n",
      "563                 NaN                            NaN   \n",
      "564                 NaN                            NaN   \n",
      "565                 NaN                            NaN   \n",
      "566                 NaN                            NaN   \n",
      "\n",
      "     Normalized_PRAC_WO_Mitigation  Normalized_PRAC_WO_Mitigation-ClosedCap4  \n",
      "0                         0.973938                                  0.975238  \n",
      "1                         0.969499                                  0.971405  \n",
      "2                         0.969759                                  0.972359  \n",
      "3                         0.969210                                  0.971288  \n",
      "4                         0.968570                                  0.970260  \n",
      "..                             ...                                       ...  \n",
      "562                       0.972367                                  0.977022  \n",
      "563                       0.973959                                  0.976619  \n",
      "564                       0.979516                                  0.997961  \n",
      "565                       0.993277                                  1.001838  \n",
      "566                       0.982943                                  0.983305  \n",
      "\n",
      "[567 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "baseline_4cores_out_path = '../../prac_study/TimingOverhead/SC/352rob'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\", \"WS\"])\n",
    "mitigation_list = [\"Baseline\", \"PRAC_WO_Mitigation\", \n",
    "                   \"Baseline-ClosedCap4\", \"PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = baseline_4cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "        interface = int(result_filename.split(\"_\")[1])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        ipc_0 = 0\n",
    "        cycle_0 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_abo=0\n",
    "        num_tREFI_period=0\n",
    "        num_tREFW_period=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" prac_num_recovery\" in line):\n",
    "                num_abo = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_command_0\" in line):\n",
    "                num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            if (\" num_refresh_window_0\" in line):\n",
    "                num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "\n",
    "        if (cycle_0 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 ):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'interface': [interface],\n",
    "            'WS': [ipc_0],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df[['workload', 'Channel', 'interface', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./stats/SC_workload_characteristics.csv', index=False)\n",
    "df_baseline = df.pivot(index=['workload', 'Channel', 'interface'], columns=['mitigation'], values='WS').reset_index()\n",
    "df_baseline[['workload', 'Channel','interface', 'Baseline','Baseline-ClosedCap4']].to_csv('./stats/SC_500M_Baseline.csv', index=False)\n",
    "\n",
    "# List of PRAC_WO_Mitigation columns and their corresponding Baseline columns\n",
    "prac_columns = [\n",
    "    'PRAC_WO_Mitigation',\n",
    "    # 'PRAC_WO_Mitigation-ClosedCap1',\n",
    "    'PRAC_WO_Mitigation-ClosedCap4'\n",
    "]\n",
    "\n",
    "baseline_columns = [\n",
    "    'Baseline',\n",
    "    # 'Baseline-ClosedCap1',\n",
    "    'Baseline-ClosedCap4'\n",
    "]\n",
    "\n",
    "# Loop through each pair of PRAC and Baseline columns to create normalized columns\n",
    "for prac_col, baseline_col in zip(prac_columns, baseline_columns):\n",
    "    normalized_col_name = f'Normalized_{prac_col}'  # Name of the new normalized column\n",
    "    df_baseline[normalized_col_name] = df_baseline[prac_col] / df_baseline[baseline_col]  # Calculate normalized values\n",
    "\n",
    "# print(df_baseline)\n",
    "\n",
    "# workload_interests = ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '482.sphinx3', '483.xalancbmk', \n",
    "#                       '500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz',\n",
    "#                       'tpcc64', 'tpch17', 'tpch2', 'tpch6',\n",
    "#                       'grep_map0', 'wc_8443', 'wc_map0',\n",
    "#                       'h264_encode', 'jp2_decode', 'jp2_encode',\n",
    "#                       'ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver',\n",
    "#                       'random_10.trace', 'stream_10.trace'\n",
    "#                       ]\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 23\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "#\n",
    "mitigation_list = [\"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap1\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            for suite_name, workloads in benchmark_suites.items():\n",
    "                suite_df = df[(df['workload'].isin(workloads)) & (df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "                if not suite_df.empty:\n",
    "                    geomeans = {}\n",
    "                    \n",
    "                    # Dynamically calculate geometric means for each mitigation\n",
    "                    for mitigation in mitigation_list:\n",
    "                        if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                            geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                    \n",
    "                    # Create a new row\n",
    "                    geomean_row = {'Channel': Channel, 'interface': interface, 'workload': suite_name, **geomeans}\n",
    "                    geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        for interface in df['interface'].unique():  # Loop through unique interfaces\n",
    "            Channel_interface_df = df[(df['Channel'] == Channel) & (df['interface'] == interface)]\n",
    "            geomean_values = {}\n",
    "\n",
    "            # Calculate geometric means for each mitigation in the list\n",
    "            for mitigation in mitigation_list:\n",
    "                if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                    geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "            # Create a new row for the combined results\n",
    "            geomean_row = {'Channel': Channel, 'interface': interface, 'workload': 'All (57)', **geomean_values}\n",
    "            geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "# Call function to calculate and merge geometric means\n",
    "geomean_df = add_geomean_rows(df_baseline)\n",
    "# geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df[['workload', 'Channel', 'interface', \"Normalized_PRAC_WO_Mitigation\",\"Normalized_PRAC_WO_Mitigation-ClosedCap4\"]].to_csv('./SC_500M_352robs_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Cores Results CAP1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_geometric_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 222\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([df, geomean_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m mitigation_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAC_WO_Mitigation-ClosedCap1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 222\u001b[0m geomean_df \u001b[38;5;241m=\u001b[39m \u001b[43madd_geomean_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_closed_cap1_ws_pivot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m geomean_df \u001b[38;5;241m=\u001b[39m add_all_workloads_geomean_rows(geomean_df)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(geomean_df)\n",
      "Cell \u001b[0;32mIn[2], line 187\u001b[0m, in \u001b[0;36madd_geomean_rows\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Dynamically calculate geometric means for each mitigation\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mitigation \u001b[38;5;129;01min\u001b[39;00m mitigation_list:\n\u001b[0;32m--> 187\u001b[0m     geomeans[mitigation] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_geometric_mean\u001b[49m(suite_df[mitigation])\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Create a new row\u001b[39;00m\n\u001b[1;32m    190\u001b[0m geomean_row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel\u001b[39m\u001b[38;5;124m'\u001b[39m: Channel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m: suite_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeomeans}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_geometric_mean' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../TimingOverhead/4cores_results'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"Baseline-ClosedCap1\", \"PRAC_WO_Mitigation-ClosedCap1\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        Channel = int(result_filename.split(\"_\")[0])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        num_rd_reqs=0\n",
    "        num_wr_reqs=0\n",
    "        wr_reqs_ratio = 0.0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_read_requests\" in line):\n",
    "                num_rd_reqs = int(line.split(\" \")[-1])\n",
    "            if (\" total_num_write_requests\" in line):\n",
    "                num_wr_reqs = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "        \n",
    "        wr_reqs_ratio = float(int(num_wr_reqs)/int(num_rd_reqs + num_wr_reqs))\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'Channel': [Channel],\n",
    "            'wl0': [w0],\n",
    "            'wl1': [w1],\n",
    "            'wl2': [w2],\n",
    "            'wl3': [w3],\n",
    "            'ipc0': [ipc_0],\n",
    "            'ipc1': [ipc_1],\n",
    "            'ipc2': [ipc_2],\n",
    "            'ipc3': [ipc_3],\n",
    "            'RD_REQs': [num_rd_reqs],\n",
    "            'WR_REQs': [num_wr_reqs],\n",
    "            'WR_ratio': [wr_reqs_ratio],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df)\n",
    "df[['workload', 'Channel', 'mitigation', 'RD_REQs', 'WR_REQs', 'WR_ratio']].to_csv('./stats/4core_heterogeneous_workload_characteristics.csv', index=False)\n",
    "df_sc_ipc = pd.read_csv('./SC_500M_Closed_Cap1_IPC.csv')\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline-ClosedCap1']], left_on=['Channel', 'wl0'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap1': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline-ClosedCap1']], left_on=['Channel', 'wl1'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap1': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline-ClosedCap1']], left_on=['Channel', 'wl2'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap1': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['Channel', 'workload_sc', 'Baseline-ClosedCap1']], left_on=['Channel', 'wl3'], right_on=['Channel', 'workload_sc'], how='left').rename(columns={'Baseline-ClosedCap1': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_closed_cap1_ws = df[['mitigation', 'workload', 'Channel', 'WS']]\n",
    "\n",
    "df_closed_cap1_ws_pivot = df_closed_cap1_ws.pivot(index=['workload', 'Channel'], columns=['mitigation'], values='WS').reset_index()\n",
    "for mitigation in set(mitigation_list) - set(['Baseline-ClosedCap1']):\n",
    "     df_closed_cap1_ws_pivot[mitigation] = df_closed_cap1_ws_pivot[mitigation] / df_closed_cap1_ws_pivot['Baseline-ClosedCap1']\n",
    "df_closed_cap1_ws_pivot.drop(columns=['Baseline-ClosedCap1'], inplace=True)\n",
    "##### Calculate the Geomean for each workload type\n",
    "# Define benchmark suites and their corresponding workloads ranges\n",
    "benchmark_types = {\n",
    "    'HHHH (15)': list(range(0, 15)),   # From Mix0-Mix14\n",
    "    'MMMM (15)': list(range(15, 30)),  # From Mix15-Mix29\n",
    "    'LLLL (15)': list(range(30, 45)),  # From Mix30-Mix44\n",
    "    'HHMM (15)': list(range(45, 60)),  # From Mix45-Mix59\n",
    "    'HHLL (15)': list(range(60, 75)),  # From Mix60-Mix74\n",
    "    'MMLL (15)': list(range(75, 90)),  # From Mix75-Mix89\n",
    "}\n",
    "\n",
    "# DataFrame Example (you already have your df)\n",
    "# Assuming your column of interest is 'PRAC_WO_Mitigation-ClosedCap1'\n",
    "\n",
    "# Add a new column to assign each Mix to the appropriate suite\n",
    "def assign_benchmark_types(mix_index):\n",
    "    for suite, mix_range in benchmark_types.items():\n",
    "        if mix_index in mix_range:\n",
    "            return suite\n",
    "    return None\n",
    "\n",
    "# Assuming 'workload' has values like 'MIX0', 'MIX1', etc., you can extract the index\n",
    "df_closed_cap1_ws_pivot['Mix_index'] = df_closed_cap1_ws_pivot['workload'].str.extract(r'(\\d+)').astype(int)  # Extract Mix number\n",
    "df_closed_cap1_ws_pivot['Benchmark_Types'] = df_closed_cap1_ws_pivot['Mix_index'].apply(assign_benchmark_types)  # Assign benchmark suite\n",
    "\n",
    "# Function to calculate the geometric mean\n",
    "def geom_mean(series):\n",
    "    return np.exp(np.log(series).mean())\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for Channel in df['Channel'].unique():\n",
    "        for suite_name, mix_indices in benchmark_types.items():\n",
    "            # Create a list of corresponding workload names (e.g., MIX0, MIX1)\n",
    "            workloads = [f'MIX{i}' for i in mix_indices]\n",
    "            suite_df = df[df['workload'].isin(workloads) & (df['Channel'] == Channel)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'Channel': Channel, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for Channel in df['Channel'].unique():\n",
    "        Channel_df = df[df['Channel'] == Channel]\n",
    "        geomean_values = {}\n",
    "        \n",
    "        # Calculate geometric means for each mitigation in the list\n",
    "        for mitigation in mitigation_list:\n",
    "            if mitigation in Channel_df.columns:  # Ensure the column exists\n",
    "                geomean_values[mitigation] = calculate_geometric_mean(Channel_df[mitigation])\n",
    "\n",
    "        # Create a new row for the combined results\n",
    "        geomean_row = {'Channel': Channel, 'workload': 'All (90)', **geomean_values}\n",
    "        geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "mitigation_list = [\"PRAC_WO_Mitigation-ClosedCap1\"]\n",
    "\n",
    "\n",
    "geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "print(geomean_df)\n",
    "geomean_df.to_csv('./4cores_500M_Closed_Cap1_Result.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
