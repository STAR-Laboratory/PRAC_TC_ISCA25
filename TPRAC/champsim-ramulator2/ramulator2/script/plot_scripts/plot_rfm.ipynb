{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**352 ROB -- Optmized MC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-core, MOP, Open-Page, RFMsb w/o Early Reset Results (PrIDE/Mint/Mithril)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './stats/SC_500M_OPTMC_Baseline.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m\n\u001b[1;32m    104\u001b[0m             new_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    105\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmitigation\u001b[39m\u001b[38;5;124m'\u001b[39m: [mitigation],\n\u001b[1;32m    106\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m: [workload],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mipc3\u001b[39m\u001b[38;5;124m'\u001b[39m: [ipc_3],\n\u001b[1;32m    116\u001b[0m             })\n\u001b[1;32m    117\u001b[0m             df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, new_row], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 119\u001b[0m df_sc_ipc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./stats/SC_500M_OPTMC_Baseline.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Choose only interested baseline\u001b[39;00m\n\u001b[1;32m    121\u001b[0m df_sc_ipc \u001b[38;5;241m=\u001b[39m df_sc_ipc[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './stats/SC_500M_OPTMC_Baseline.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../../prac_study/rfm_results/optimized_MC/results_4cores'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "df_baseline = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"RFMsb\", \"RFMpb\", \"Baseline\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        workload = result_filename\n",
    "\n",
    "        if mitigation not in ['Baseline']:\n",
    "            BAT = int(result_filename.split(\"_\")[0])\n",
    "            workload = \"_\".join(result_filename.split(\"_\")[1:])\n",
    "\n",
    "\n",
    "        # workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        if mitigation in [\"Baseline\"]:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df_baseline = pd.concat([df_baseline, new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'BAT': [BAT],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_sc_ipc = pd.read_csv('../stats/SC_500M_OPTMC_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel']==1) & (df_sc_ipc['interface'] == 6400)]\n",
    "df_sc_ipc = df_sc_ipc.drop(columns=['Channel', 'interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df_baseline['normalzied_ipc0'] = df_baseline['ipc0'] / df_baseline['ipc_wl0']\n",
    "df_baseline['normalzied_ipc1'] = df_baseline['ipc1'] / df_baseline['ipc_wl1']\n",
    "df_baseline['normalzied_ipc2'] = df_baseline['ipc2'] / df_baseline['ipc_wl2']\n",
    "df_baseline['normalzied_ipc3'] = df_baseline['ipc3'] / df_baseline['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "df_baseline['WS'] = df_baseline[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_ws_openpage = df[['mitigation', 'BAT', 'workload','WS']]\n",
    "df_baseline_ws_openpage = df_baseline[['mitigation', 'workload','WS']]\n",
    "# print(df_baseline_ws_openpage)\n",
    "# print(df_ws_openpage)\n",
    "\n",
    "# Merge df_ws with df_baseline_ws on the 'workload' column\n",
    "merged_df = pd.merge(df_ws_openpage, df_baseline_ws_openpage[['workload', 'WS']], on='workload', how='left', suffixes=('', '_baseline'))\n",
    "# Calculate normalized WS\n",
    "merged_df['normalized_WS'] = merged_df['WS'] / merged_df['WS_baseline']\n",
    "\n",
    "# Display the result\n",
    "# print(merged_df[['mitigation', 'BAT', 'workload', 'normalized_WS']])\n",
    "merged_df_pivot = merged_df.pivot(index=['workload', 'BAT'], columns=['mitigation'], values='normalized_WS').reset_index()\n",
    "\n",
    "# print(merged_df_pivot)\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 22\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    # 'LonestarGPU (3)': ['bfs_dblp', 'bfsbfs_cm2003_cm_2003','bfs_ny'], #lnestargpu: 3\n",
    "    # 'MediaBench (4)': ['h264_decode', 'h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 4\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for BAT in df['BAT'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['BAT'] == BAT)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'BAT': BAT, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for BAT in df['BAT'].unique():\n",
    "        Channel_interface_df = df[(df['BAT'] == BAT)]\n",
    "        geomean_values = {}\n",
    "\n",
    "        # Calculate geometric means for each mitigation in the list\n",
    "        for mitigation in mitigation_list:\n",
    "            if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "        # Create a new row for the combined results\n",
    "        geomean_row = {'BAT': BAT, 'workload': 'All (57)', **geomean_values}\n",
    "        geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Call function to calculate and merge geometric means\n",
    "mitigation_list = ['RFMsb', 'RFMpb']\n",
    "geomean_rfm_reset_ws = add_geomean_rows(merged_df_pivot)\n",
    "geomean_rfm_reset_ws = add_all_workloads_geomean_rows(geomean_rfm_reset_ws)\n",
    "print(geomean_rfm_reset_ws)\n",
    "geomean_rfm_reset_ws.to_csv('../stats/RFMsb_NoReset_500M_4homogeneous_OptimizedMC.csv', index=False)\n",
    "# geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "# geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "# print(geomean_df)\n",
    "# geomean_df.to_csv('./stats/RFM_500M_4homogeneous.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-core, MOP, Open-Page, RFM w/ Early Reset Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      workload    BAT  RFMab-Reset  RFMsb-Reset\n",
      "0    401.bzip2   40.0     0.980184     0.995539\n",
      "1    401.bzip2   70.0     0.997686     1.003284\n",
      "2    401.bzip2  100.0     0.993785     0.999167\n",
      "3    401.bzip2  140.0     0.992330     1.007254\n",
      "4      403.gcc   40.0     0.997925     0.999310\n",
      "..         ...    ...          ...          ...\n",
      "251   YCSB (6)  140.0     0.993203     0.996724\n",
      "252   All (57)   40.0     0.958309     0.978668\n",
      "253   All (57)   70.0     0.977934     0.988346\n",
      "254   All (57)  100.0     0.985079     0.991961\n",
      "255   All (57)  140.0     0.989491     0.994372\n",
      "\n",
      "[256 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../rfm_results/optimized_MC/results_4cores'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "df_baseline = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"RFMab-Reset\", \"RFMsb-Reset\", \"Baseline\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        workload = result_filename\n",
    "\n",
    "        if mitigation not in ['Baseline']:\n",
    "            BAT = int(result_filename.split(\"_\")[0])\n",
    "            workload = \"_\".join(result_filename.split(\"_\")[1:])\n",
    "\n",
    "\n",
    "        # workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        if mitigation in [\"Baseline\"]:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df_baseline = pd.concat([df_baseline, new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'BAT': [BAT],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df_baseline)\n",
    "df_sc_ipc = pd.read_csv('./stats/SC_500M_OPTMC_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel']==1) & (df_sc_ipc['interface'] == 6400)]\n",
    "df_sc_ipc = df_sc_ipc.drop(columns=['Channel', 'interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df_baseline['normalzied_ipc0'] = df_baseline['ipc0'] / df_baseline['ipc_wl0']\n",
    "df_baseline['normalzied_ipc1'] = df_baseline['ipc1'] / df_baseline['ipc_wl1']\n",
    "df_baseline['normalzied_ipc2'] = df_baseline['ipc2'] / df_baseline['ipc_wl2']\n",
    "df_baseline['normalzied_ipc3'] = df_baseline['ipc3'] / df_baseline['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "df_baseline['WS'] = df_baseline[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_ws_openpage = df[['mitigation', 'BAT', 'workload','WS']]\n",
    "df_baseline_ws_openpage = df_baseline[['mitigation', 'workload','WS']]\n",
    "# print(df_baseline_ws_openpage)\n",
    "# print(df_ws_openpage)\n",
    "\n",
    "# Merge df_ws with df_baseline_ws on the 'workload' column\n",
    "merged_df = pd.merge(df_ws_openpage, df_baseline_ws_openpage[['workload', 'WS']], on='workload', how='left', suffixes=('', '_baseline'))\n",
    "# Calculate normalized WS\n",
    "merged_df['normalized_WS'] = merged_df['WS'] / merged_df['WS_baseline']\n",
    "\n",
    "# Display the result\n",
    "# print(merged_df[['mitigation', 'BAT', 'workload', 'normalized_WS']])\n",
    "merged_df_pivot = merged_df.pivot(index=['workload', 'BAT'], columns=['mitigation'], values='normalized_WS').reset_index()\n",
    "\n",
    "# print(merged_df_pivot)\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 22\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    # 'LonestarGPU (3)': ['bfs_dblp', 'bfsbfs_cm2003_cm_2003','bfs_ny'], #lnestargpu: 3\n",
    "    # 'MediaBench (4)': ['h264_decode', 'h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 4\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for BAT in df['BAT'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['BAT'] == BAT)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'BAT': BAT, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for BAT in df['BAT'].unique():\n",
    "        Channel_interface_df = df[(df['BAT'] == BAT)]\n",
    "        geomean_values = {}\n",
    "\n",
    "        # Calculate geometric means for each mitigation in the list\n",
    "        for mitigation in mitigation_list:\n",
    "            if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "        # Create a new row for the combined results\n",
    "        geomean_row = {'BAT': BAT, 'workload': 'All (57)', **geomean_values}\n",
    "        geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Call function to calculate and merge geometric means\n",
    "mitigation_list = ['RFMab-Reset', 'RFMsb-Reset']\n",
    "geomean_rfm_reset_ws = add_geomean_rows(merged_df_pivot)\n",
    "geomean_rfm_reset_ws = add_all_workloads_geomean_rows(geomean_rfm_reset_ws)\n",
    "print(geomean_rfm_reset_ws)\n",
    "geomean_rfm_reset_ws.to_csv('./stats/RFM_Reset_500M_4homogeneous_OptimizedMC.csv', index=False)\n",
    "# geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "# geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "# print(geomean_df)\n",
    "# geomean_df.to_csv('./stats/RFM_500M_4homogeneous.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**352 ROB -- Non Optmized MC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Cores, RoBaRaCoCh, Open-Page, RFMsb w/o Early Reset Results (PrIDE/Mint/Mithril)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mitigation      workload   BAT     RFMsb\n",
      "0              401.bzip2   1.0  0.312663\n",
      "1              401.bzip2   2.0  0.790512\n",
      "2              401.bzip2   5.0  1.579479\n",
      "3              401.bzip2  10.0  2.081050\n",
      "4              401.bzip2  40.0  2.705606\n",
      "..                   ...   ...       ...\n",
      "333         ycsb_eserver   2.0  0.416433\n",
      "334         ycsb_eserver   5.0  0.991393\n",
      "335         ycsb_eserver  10.0  1.540306\n",
      "336         ycsb_eserver  40.0  2.392663\n",
      "337         ycsb_eserver  70.0  2.575094\n",
      "\n",
      "[338 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../rfm_results/results_4cores'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"RFMsb\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        BAT = int(result_filename.split(\"_\")[0])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[1:])\n",
    "        # workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\n",
    "            'mitigation': [mitigation],\n",
    "            'workload': [workload],\n",
    "            'BAT': [BAT],\n",
    "            'wl0': [w0],\n",
    "            'wl1': [w1],\n",
    "            'wl2': [w2],\n",
    "            'wl3': [w3],\n",
    "            'ipc0': [ipc_0],\n",
    "            'ipc1': [ipc_1],\n",
    "            'ipc2': [ipc_2],\n",
    "            'ipc3': [ipc_3],\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# print(df)\n",
    "df_sc_ipc = pd.read_csv('./stats/SC_500M_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel']==1) & (df_sc_ipc['interface'] == 6400)]\n",
    "df_sc_ipc = df_sc_ipc.drop(columns=['Channel', 'interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_rfm_methods_ws_openpage = df[['mitigation', 'BAT', 'workload','WS']]\n",
    "df_rfm_methods_ws_open_pivot = df_rfm_methods_ws_openpage.pivot(index=['workload', 'BAT'], columns=['mitigation'], values='WS').reset_index()\n",
    "print(df_rfm_methods_ws_open_pivot)\n",
    "# for mitigation in set(mitigation_list) - set(['Baseline']):\n",
    "#      df_ws_openpage_pivot[mitigation] = df_ws_openpage_pivot[mitigation] / df_ws_openpage_pivot['Baseline']\n",
    "# df_ws_openpage_pivot.drop(columns=['Baseline'], inplace=True)\n",
    "\n",
    "# print(df_ws_openpage)\n",
    "# ##### Calculate the Geomean for each workload type\n",
    "# # Define benchmark suites and their corresponding workloads ranges\n",
    "# benchmark_types = {\n",
    "#     'HHHH (15)': list(range(0, 15)),   # From Mix0-Mix14\n",
    "#     'MMMM (15)': list(range(15, 30)),  # From Mix15-Mix29\n",
    "#     'LLLL (15)': list(range(30, 45)),  # From Mix30-Mix44\n",
    "#     'HHMM (15)': list(range(45, 60)),  # From Mix45-Mix59\n",
    "#     'HHLL (15)': list(range(60, 75)),  # From Mix60-Mix74\n",
    "#     'MMLL (15)': list(range(75, 90)),  # From Mix75-Mix89\n",
    "# }\n",
    "\n",
    "# # DataFrame Example (you already have your df)\n",
    "# # Assuming your column of interest is 'PRAC_WO_Mitigation-ClosedCap1'\n",
    "\n",
    "# # Add a new column to assign each Mix to the appropriate suite\n",
    "# def assign_benchmark_types(mix_index):\n",
    "#     for suite, mix_range in benchmark_types.items():\n",
    "#         if mix_index in mix_range:\n",
    "#             return suite\n",
    "#     return None\n",
    "\n",
    "# # Assuming 'workload' has values like 'MIX0', 'MIX1', etc., you can extract the index\n",
    "# df_closed_cap1_ws_pivot['Mix_index'] = df_closed_cap1_ws_pivot['workload'].str.extract(r'(\\d+)').astype(int)  # Extract Mix number\n",
    "# df_closed_cap1_ws_pivot['Benchmark_Types'] = df_closed_cap1_ws_pivot['Mix_index'].apply(assign_benchmark_types)  # Assign benchmark suite\n",
    "\n",
    "# # Function to calculate the geometric mean\n",
    "# def geom_mean(series):\n",
    "#     return np.exp(np.log(series).mean())\n",
    "\n",
    "# # Function to calculate and add geometric means as new rows\n",
    "# def add_geomean_rows(df):\n",
    "#     geomean_rows = []  # List to collect new rows\n",
    "\n",
    "#     for Channel in df['Channel'].unique():\n",
    "#         for suite_name, mix_indices in benchmark_types.items():\n",
    "#             # Create a list of corresponding workload names (e.g., MIX0, MIX1)\n",
    "#             workloads = [f'MIX{i}' for i in mix_indices]\n",
    "#             suite_df = df[df['workload'].isin(workloads) & (df['Channel'] == Channel)]\n",
    "#             if not suite_df.empty:\n",
    "#                 geomeans = {}\n",
    "                \n",
    "#                 # Dynamically calculate geometric means for each mitigation\n",
    "#                 for mitigation in mitigation_list:\n",
    "#                     geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "#                 # Create a new row\n",
    "#                 geomean_row = {'Channel': Channel, 'workload': suite_name, **geomeans}\n",
    "#                 geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "#     # Convert list of rows to DataFrame\n",
    "#     geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "#     return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# def add_all_workloads_geomean_rows(df):\n",
    "#     geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "#     for Channel in df['Channel'].unique():\n",
    "#         Channel_df = df[df['Channel'] == Channel]\n",
    "#         geomean_values = {}\n",
    "        \n",
    "#         # Calculate geometric means for each mitigation in the list\n",
    "#         for mitigation in mitigation_list:\n",
    "#             if mitigation in Channel_df.columns:  # Ensure the column exists\n",
    "#                 geomean_values[mitigation] = calculate_geometric_mean(Channel_df[mitigation])\n",
    "\n",
    "#         # Create a new row for the combined results\n",
    "#         geomean_row = {'Channel': Channel, 'workload': 'All (90)', **geomean_values}\n",
    "#         geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "#     # Convert list of rows to DataFrame\n",
    "#     geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "#     return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# mitigation_list = [\"PRAC_WO_Mitigation-ClosedCap1\"]\n",
    "\n",
    "\n",
    "# geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "# geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "# print(geomean_df)\n",
    "# geomean_df.to_csv('./stats/RFM_500M_4homogeneous.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Cores, MOP Mapping, RFM with Early Reset Results OpenRowPolicy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      workload    BAT  RFMab-Reset  RFMsb-Reset\n",
      "0    401.bzip2   40.0     0.986082     0.973553\n",
      "1    401.bzip2   70.0     0.988595     0.981370\n",
      "2    401.bzip2  100.0     0.993958     0.985553\n",
      "3    401.bzip2  140.0     0.986823     0.987618\n",
      "4      403.gcc   40.0     0.999423     0.998078\n",
      "..         ...    ...          ...          ...\n",
      "251   YCSB (6)  140.0     0.996113     0.993336\n",
      "252   All (57)   40.0     0.973328     0.951822\n",
      "253   All (57)   70.0     0.987317     0.973465\n",
      "254   All (57)  100.0     0.991135     0.982107\n",
      "255   All (57)  140.0     0.993769     0.987487\n",
      "\n",
      "[256 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "multi_cores_out_path = '../rfm_results/results_4cores'\n",
    "\n",
    "df = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "df_baseline = pd.DataFrame(columns=[\"mitigation\", \"workload\"])\n",
    "mitigation_list = [\"RFMab-Reset\", \"RFMsb-Reset\", \"Baseline\"]\n",
    "# mitigation_list = [\"Baseline-ClosedCap1\"]\n",
    "for mitigation in mitigation_list:\n",
    "    result_path = multi_cores_out_path + \"/\" + mitigation +\"/stats/\"\n",
    "    result_list = [x[:-4] for x in os.listdir(result_path) if x.endswith(\".txt\")]\n",
    "    for result_filename in result_list:\n",
    "        result_file = open(result_path + result_filename + \".txt\", \"r\")\n",
    "        if mitigation not in ['Baseline']:\n",
    "            BAT = int(result_filename.split(\"_\")[0])\n",
    "\n",
    "        workload = \"_\".join(result_filename.split(\"_\")[1:])\n",
    "        # workload = \"_\".join(result_filename.split(\"_\")[2:])\n",
    "\n",
    "        w0=''\n",
    "        w1=''\n",
    "        w2=''\n",
    "        w3=''\n",
    "        ipc_0 = 0\n",
    "        ipc_1 = 0\n",
    "        ipc_2 = 0\n",
    "        ipc_3 = 0\n",
    "        cycle_0 = 0\n",
    "        cycle_1 = 0\n",
    "        cycle_2 = 0\n",
    "        cycle_3 = 0\n",
    "        num_inst_0=0\n",
    "        num_inst_1=0\n",
    "        num_inst_2=0\n",
    "        num_inst_3=0\n",
    "        # num_tREFI_period=0\n",
    "        # num_tREFW_period=0\n",
    "        for line in result_file.readlines():\n",
    "            if (\"name_trace_0:\" in line):\n",
    "                w0 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_1:\" in line):\n",
    "                w1 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_2:\" in line):\n",
    "                w2 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\"name_trace_3:\" in line):\n",
    "                w3 = str(line.split(\"/\")[-1]).strip()\n",
    "            if (\" cycles_recorded_core_0:\" in line):\n",
    "                cycle_0 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_1:\" in line):\n",
    "                cycle_1 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_2:\" in line):\n",
    "                cycle_2 = int(line.split(\" \")[-1])\n",
    "            if (\" cycles_recorded_core_3:\" in line):\n",
    "                cycle_3 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_0\" in line):\n",
    "                num_inst_0 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_1\" in line):\n",
    "                num_inst_1 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_2\" in line):\n",
    "                num_inst_2 = int(line.split(\" \")[-1])\n",
    "            if (\" insts_recorded_core_3\" in line):\n",
    "                num_inst_3 = int(line.split(\" \")[-1])\n",
    "            # if (\" prac_num_recovery\" in line):\n",
    "            #     num_abo = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_command_0\" in line):\n",
    "            #     num_tREFI_period = int(line.split(\" \")[-1])\n",
    "            # if (\" num_refresh_window_0\" in line):\n",
    "            #     num_tREFW_period = int(line.split(\" \")[-1])            \n",
    "                \n",
    "        if (cycle_0 == 0 and cycle_1 == 0 and cycle_2 == 0 and cycle_3 == 0):\n",
    "            continue\n",
    "        if (cycle_0 == 0 or cycle_1 == 0 or cycle_2 == 0 or cycle_3 == 0):\n",
    "            print(\"Error: \" + result_filename)\n",
    "        ipc_0 = int(num_inst_0) / cycle_0\n",
    "        ipc_1 = int(num_inst_1) / cycle_1\n",
    "        ipc_2 = int(num_inst_2) / cycle_2\n",
    "        ipc_3 = int(num_inst_3) / cycle_3\n",
    "\n",
    "        result_file.close()\n",
    "        # Create a new DataFrame for the new row\n",
    "        if mitigation in [\"Baseline\"]:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df_baseline = pd.concat([df_baseline, new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\n",
    "                'mitigation': [mitigation],\n",
    "                'workload': [workload],\n",
    "                'BAT': [BAT],\n",
    "                'wl0': [w0],\n",
    "                'wl1': [w1],\n",
    "                'wl2': [w2],\n",
    "                'wl3': [w3],\n",
    "                'ipc0': [ipc_0],\n",
    "                'ipc1': [ipc_1],\n",
    "                'ipc2': [ipc_2],\n",
    "                'ipc3': [ipc_3],\n",
    "            })\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df_sc_ipc = pd.read_csv('./stats/SC_500M_Baseline.csv')\n",
    "# Choose only interested baseline\n",
    "df_sc_ipc = df_sc_ipc[['workload', 'Channel', 'interface', 'Baseline']]\n",
    "df_sc_ipc = df_sc_ipc[(df_sc_ipc['Channel']==1) & (df_sc_ipc['interface'] == 6400)]\n",
    "df_sc_ipc = df_sc_ipc.drop(columns=['Channel', 'interface'])\n",
    "df_sc_ipc = df_sc_ipc.rename(columns={'workload': 'workload_sc'})\n",
    "# print(df_sc_ipc)\n",
    "\n",
    "# First, merge df with df_sc_ipc for each workload (wl0, wl1, wl2, wl3)\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl0'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl0'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl1'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl1'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl2'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl2'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Drop 'workload' again if it exists\n",
    "\n",
    "df = df.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "df_baseline = df_baseline.merge(df_sc_ipc[['workload_sc', 'Baseline']], left_on=['wl3'], right_on=['workload_sc'], how='left').rename(columns={'Baseline': 'ipc_wl3'})\n",
    "if 'workload_sc' in df.columns:\n",
    "    df = df.drop(columns=['workload_sc'])  # Final cleanup\n",
    "    df_baseline = df_baseline.drop(columns=['workload_sc'])  # Final cleanup\n",
    "\n",
    "df['normalzied_ipc0'] = df['ipc0'] / df['ipc_wl0']\n",
    "df['normalzied_ipc1'] = df['ipc1'] / df['ipc_wl1']\n",
    "df['normalzied_ipc2'] = df['ipc2'] / df['ipc_wl2']\n",
    "df['normalzied_ipc3'] = df['ipc3'] / df['ipc_wl3']\n",
    "\n",
    "df_baseline['normalzied_ipc0'] = df_baseline['ipc0'] / df_baseline['ipc_wl0']\n",
    "df_baseline['normalzied_ipc1'] = df_baseline['ipc1'] / df_baseline['ipc_wl1']\n",
    "df_baseline['normalzied_ipc2'] = df_baseline['ipc2'] / df_baseline['ipc_wl2']\n",
    "df_baseline['normalzied_ipc3'] = df_baseline['ipc3'] / df_baseline['ipc_wl3']\n",
    "\n",
    "df['WS'] = df[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "df_baseline['WS'] = df_baseline[['normalzied_ipc0', 'normalzied_ipc1', 'normalzied_ipc2', 'normalzied_ipc3']].sum(axis=1)\n",
    "\n",
    "df_ws_openpage = df[['mitigation', 'BAT', 'workload','WS']]\n",
    "df_baseline_ws_openpage = df_baseline[['mitigation', 'workload','WS']]\n",
    "# print(df_baseline_ws_openpage)\n",
    "# print(df_ws_openpage)\n",
    "\n",
    "# Merge df_ws with df_baseline_ws on the 'workload' column\n",
    "merged_df = pd.merge(df_ws_openpage, df_baseline_ws_openpage[['workload', 'WS']], on='workload', how='left', suffixes=('', '_baseline'))\n",
    "# Calculate normalized WS\n",
    "merged_df['normalized_WS'] = merged_df['WS'] / merged_df['WS_baseline']\n",
    "\n",
    "# Display the result\n",
    "# print(merged_df[['mitigation', 'BAT', 'workload', 'normalized_WS']])\n",
    "merged_df_pivot = merged_df.pivot(index=['workload', 'BAT'], columns=['mitigation'], values='normalized_WS').reset_index()\n",
    "\n",
    "# print(merged_df_pivot)\n",
    "\n",
    "# Define benchmark suites and their corresponding workloads\n",
    "benchmark_suites = {\n",
    "    'SPEC2K6 (23)': ['401.bzip2', '403.gcc', '429.mcf', '433.milc', '434.zeusmp', '435.gromacs', '436.cactusADM', '437.leslie3d', '444.namd', '445.gobmk', '447.dealII', '450.soplex', '456.hmmer', '458.sjeng', '459.GemsFDTD', '462.libquantum', '464.h264ref', '470.lbm', '471.omnetpp', '473.astar', '481.wrf', '482.sphinx3', '483.xalancbmk'], # SPEC2K6: 22\n",
    "    'SPEC2K17 (18)': ['500.perlbench', '502.gcc', '505.mcf', '507.cactuBSSN', '508.namd', '510.parest', '511.povray', '519.lbm', '520.omnetpp', '523.xalancbmk', '525.x264', '526.blender', '531.deepsjeng', '538.imagick', '541.leela', '544.nab', '549.fotonik3d', '557.xz'], # SPEC2K17: 18\n",
    "    'TPC (4)': ['tpcc64', 'tpch17', 'tpch2', 'tpch6'], #tpc: 4\n",
    "    # TODO: Enable Hadoop and LonestartGPU after fixing the performance shooting problem + h264_decode\n",
    "    'Hadoop (3)': ['grep_map0', 'wc_8443', 'wc_map0'], #Hadoop: 3\n",
    "    # 'LonestarGPU (3)': ['bfs_dblp', 'bfsbfs_cm2003_cm_2003','bfs_ny'], #lnestargpu: 3\n",
    "    # 'MediaBench (4)': ['h264_decode', 'h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 4\n",
    "    'MediaBench (3)': ['h264_encode', 'jp2_decode', 'jp2_encode'], #mediabench: 3\n",
    "    'YCSB (6)': ['ycsb_abgsave', 'ycsb_aserver', 'ycsb_bserver', 'ycsb_cserver', 'ycsb_dserver', 'ycsb_eserver'] #ycsb:6\n",
    "}\n",
    "\n",
    "# Function to calculate geometric mean\n",
    "def calculate_geometric_mean(series):\n",
    "    return np.prod(series) ** (1 / len(series))\n",
    "\n",
    "# Function to calculate and add geometric means as new rows\n",
    "def add_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "\n",
    "    for BAT in df['BAT'].unique():\n",
    "        for suite_name, workloads in benchmark_suites.items():\n",
    "            suite_df = df[(df['workload'].isin(workloads)) & (df['BAT'] == BAT)]\n",
    "            if not suite_df.empty:\n",
    "                geomeans = {}\n",
    "                \n",
    "                # Dynamically calculate geometric means for each mitigation\n",
    "                for mitigation in mitigation_list:\n",
    "                    if mitigation in suite_df.columns:  # Ensure the column exists\n",
    "                        geomeans[mitigation] = calculate_geometric_mean(suite_df[mitigation])\n",
    "                \n",
    "                # Create a new row\n",
    "                geomean_row = {'BAT': BAT, 'workload': suite_name, **geomeans}\n",
    "                geomean_rows.append(geomean_row)  # Append to the list\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Function to add combined geometric means for all workloads in each channel and interface\n",
    "def add_all_workloads_geomean_rows(df):\n",
    "    geomean_rows = []  # List to collect new rows\n",
    "    \n",
    "    for BAT in df['BAT'].unique():\n",
    "        Channel_interface_df = df[(df['BAT'] == BAT)]\n",
    "        geomean_values = {}\n",
    "\n",
    "        # Calculate geometric means for each mitigation in the list\n",
    "        for mitigation in mitigation_list:\n",
    "            if mitigation in Channel_interface_df.columns:  # Ensure the column exists\n",
    "                geomean_values[mitigation] = calculate_geometric_mean(Channel_interface_df[mitigation])\n",
    "\n",
    "        # Create a new row for the combined results\n",
    "        geomean_row = {'BAT': BAT, 'workload': 'All (57)', **geomean_values}\n",
    "        geomean_rows.append(geomean_row)  # Append to the list\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    geomean_df = pd.DataFrame(geomean_rows)\n",
    "    \n",
    "    return pd.concat([df, geomean_df], ignore_index=True)\n",
    "\n",
    "# Call function to calculate and merge geometric means\n",
    "mitigation_list = ['RFMab-Reset', 'RFMsb-Reset']\n",
    "geomean_rfm_reset_ws = add_geomean_rows(merged_df_pivot)\n",
    "geomean_rfm_reset_ws = add_all_workloads_geomean_rows(geomean_rfm_reset_ws)\n",
    "print(geomean_rfm_reset_ws)\n",
    "geomean_rfm_reset_ws.to_csv('./stats/RFM_Reset_500M_4homogeneous.csv', index=False)\n",
    "# geomean_df = add_geomean_rows(df_closed_cap1_ws_pivot)\n",
    "# geomean_df = add_all_workloads_geomean_rows(geomean_df)\n",
    "# print(geomean_df)\n",
    "# geomean_df.to_csv('./stats/RFM_500M_4homogeneous.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
